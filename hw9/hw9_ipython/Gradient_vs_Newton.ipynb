{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(x_array):\n",
    "    x_array = np.array(x_array)\n",
    "    plt.scatter(x_array[:,0], x_array[:,1], s = 5)\n",
    "    plt.plot(x_array[:,0],x_array[:,1], 'o-', zorder=2) \n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_comparison(x_grad, x_newton):\n",
    "    x_grad = np.array(x_grad)\n",
    "    x_newton = np.array(x_newton)\n",
    "    plt.scatter(x_grad[:,0], x_grad[:,1], s = 5)\n",
    "    plt.plot(x_grad[:,0],x_grad[:,1], 'o-', zorder=2,  label = 'Gradient Descent')    \n",
    "    plt.scatter(x_newton[:,0], x_newton[:,1], s = 5,)\n",
    "    plt.plot(x_newton[:,0],x_newton[:,1], 'o-', zorder=2,  label = \"Newton Raphson method\") \n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimization functions (Gradient Descent and Newton-Raphson Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimization algorithms\n",
    "#Gradient descent (first order method)\n",
    "def gradient_descent(gradient,x_init, stepsize, eps = 1e-6):\n",
    "    max_iters = 1000\n",
    "    x_array = [x_init]\n",
    "    x = x_init\n",
    "    num_iters =1\n",
    "    prev_x = float('Inf')\n",
    "    while num_iters < max_iters:\n",
    "        x =  #TODO\n",
    "        x_array.append(x) \n",
    "        if np.linalg.norm (x - prev_x) < eps: #We stop if the change in x is below a certain threshold\n",
    "            break\n",
    "        num_iters += 1\n",
    "        prev_x = x\n",
    "    \n",
    "    return x_array, num_iters\n",
    "\n",
    "#Netwon's method (second order method)\n",
    "def Newton(gradient,Hessian, x_init,  stepsize=1.0, eps = 1e-6):\n",
    "    max_iters = 1000\n",
    "    x_array = [x_init]\n",
    "    x = x_init\n",
    "    num_iters =1\n",
    "    prev_x = float('Inf')\n",
    "    while num_iters < max_iters:\n",
    "        x =  #TODO\n",
    "        x_array.append(x) \n",
    "        if np.linalg.norm (x - prev_x) < eps:  #We stop if the change in x is below a certain threshold\n",
    "            num_iters -= 1\n",
    "            break\n",
    "        num_iters += 1\n",
    "        prev_x = x\n",
    "    \n",
    "    return x_array, num_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing a quadratic\n",
    "Consider the following unconstrained convex optimization problem,\n",
    "\\begin{aligned}\n",
    "\\min_{x_0,x_1 \\in \\mathbb{R}} f(x_0,x_1) =  \\frac{1}{2} \\left (32x_0^2 + x_1^2 \\right)\n",
    "\\end{aligned}\n",
    "\n",
    "Clearly, the optimal value of the problem is $0$ and $x_0^* = x_1^*= 0$.\n",
    "\n",
    "## Using gradient descent\n",
    "\n",
    "### Part a) Write the gradient expressions for $f(x)$, where $x = [x_0, x_1]^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    '''Return f(x)'''\n",
    "    fx =  #TODO define the function\n",
    "    return fx\n",
    "\n",
    "def gradient(x):\n",
    "    '''Return gradient at x'''\n",
    "    grad  = np.array(#TODO).astype('float') #TODO define the gradient inside the np.array()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note that we can write the objective as $\\frac{1}{2}x^\\top Q x$ where $Q = \\begin{bmatrix}32 & 0 \\\\0 & 1 \\end{bmatrix}$, according to the standard form of a convex QP. Note that the largest eigenvalue of $Q$ is given by $\\lambda_{max}(Q) = 32$. \n",
    "The maximum stepsize for convergence is $\\left(\\frac{2}{\\lambda_{max}(Q)}\\right)$.\n",
    "\n",
    "Suppose you start with $x_0 = [0.1,1]$. \n",
    "\n",
    "### Part b) Run gradient descent for the following stepsizes ($\\eta$) with 1000 iterations and compare the paths traced by $x$:\n",
    "1. $\\eta = \\frac{2}{31.9}$\n",
    "2. $\\eta = \\frac{2}{35}$\n",
    "3. $\\eta = \\frac{2}{128}$\n",
    "\n",
    "\n",
    "And note your observation for each of the parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta =2/31.9\n",
    "x0 = np.array([0.1,1])\n",
    "x_array, num_iters =  #TODO run gradient descent on f(x)\n",
    "visualize(x_array)\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Note your observations here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 2/35\n",
    "x0 = np.array([0.1,1])\n",
    "x_array, num_iters = #TODO run gradient descent on f(x)\n",
    "visualize(x_array)\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Note your observations here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta= 2/128\n",
    "x0 = np.array([0.1,1])\n",
    "x_array, num_iters =  #TODO run gradient descent on f(x)\n",
    "x_grad = x_array\n",
    "visualize(x_array)\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Note your observations here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Newton's method\n",
    "Next we will use Newton's method to see if convergence is faster. \n",
    "### Part c) Write the expression for hessian of $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hessian(x):\n",
    "    '''Return Hessian at x'''\n",
    "    H = np.array(#TODO).astype('float') #TODO write the expression for hessian inside np.array()\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton Raphson's method in action\n",
    "Suppose you start with $x_0 = [0.1,1]$. \n",
    "\n",
    "We don't provide an explicit expression for largest allowed stepsize for Newton's method, but observe convergence properties using different step-sizes. \n",
    "### Part d) Run Newton Raphson's method for the following stepsizes and compare the paths traced by $x_k$:\n",
    "1. $\\eta = 2.2$\n",
    "2. $\\eta =1$\n",
    "3. $\\eta = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.1,1])\n",
    "eta = 2.2\n",
    "x_array, num_iters =  #TODO run newton-raphson on f(x)\n",
    "visualize(x_array)\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Note your observations here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.1,1])\n",
    "eta = 1.0\n",
    "x_array, num_iters =  #TODO run newton-raphson on f(x)\n",
    "visualize(x_array)\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Note your observations here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.1,1])\n",
    "eta = 0.5\n",
    "x_array, num_iters =  #TODO run newton-raphson on f(x)\n",
    "visualize(x_array)\n",
    "x_newton = x_array\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Note your observations here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e) Compare the paths taken by gradient descent with stepsize 2/128 and Newton's method with stepsize 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "visualize_comparison(x_grad, x_newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Note your observations here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing a non-quadratic objective\n",
    "Next we consider a problem that involves minimization of an objective function that is not quadratic,\n",
    "\n",
    "\\begin{aligned}\n",
    "\\min_{x \\in \\mathbb{R}} f(x_1,x_2) =  \\frac{1}{2} \\left (10x_1^2 + x_2^2 \\right) +  5\\log(1 + e^{-x_1 -x_2})\n",
    "\\end{aligned}\n",
    "where, $x = [x_1,x_2]^\\top$\n",
    "\n",
    "We can write the update equations for both gradient descent and Newton's method for this case using the same approach that we used for the previous case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient2(x):\n",
    "    '''Return gradient at x'''\n",
    "     #TODO write expression for gradient of f(x) inside np.array()\n",
    "    grad  = np.array(#TODO).astype('float')\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "def Hessian2(x):\n",
    "    '''Return Hessian at x''' #TODO write expression for hessian of f(x) inside np.array()\n",
    "    H = np.array(#TODO).astype('float')\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in action\n",
    "Suppose you start with $x_0 = [-20,-20]$. \n",
    "\n",
    "### Part f) Run gradient descent using stepsize of 1/8 and plot the trajectory as well as optimal value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta= 1/8\n",
    "x0 = np.array([-20,-20])\n",
    "x_array, num_iters = #TODO run gradient descent on f(x)\n",
    "x_grad = x_array\n",
    "visualize(x_array)\n",
    "x_grad2 = x_array\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method in action\n",
    "Suppose you start with $x_0 = [20,20]$. \n",
    "\n",
    "### Part g) Run Newton's method using stepsize of 1 and 1/8 and plot the trajectory as well as optimal value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-20,-20])\n",
    "eta = 1.0\n",
    "x_array, num_iters = #TODO run newton raphson on f(x)\n",
    "visualize(x_array)\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Note your observations here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-20,-20])\n",
    "eta = 1.0/8.0\n",
    "x_array, num_iters = #TODO run newton raphson on f(x)\n",
    "visualize(x_array)\n",
    "x_newton2 = x_array\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part h) Compare the paths taken by gradient descent with stepsize 1/8 and Newton's method with stepsize 1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "visualize_comparison(x_grad2, x_newton2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Note your observations here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with stepsizes for gradient descent and Newton's method in the cells above and see when things start to diverge and how the paths taken evolve as stepsize changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
