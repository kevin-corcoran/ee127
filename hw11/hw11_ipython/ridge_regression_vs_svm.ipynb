{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Vs. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we compare linear SVM and Ridge Regression in the task of classification. As we shall see, formulating the problem as different optimization problems (here SVM and Ridge Regression) makes a difference in performance. There are three places with todos, follow the todos to complete this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for visualization. No todo here. \n",
    "# Usage: plot_boundry(X, y, fitted_model)\n",
    "#     X: your features, where each row is a data sample\n",
    "#     y: your labels, can be 0/1 or -1/1\n",
    "#     fitted_model: a scipy TRAINED model, such as sklearn.svm.SVC\n",
    "#\n",
    "def plot_boundry(X, y, fitted_model):\n",
    "    \n",
    "    plt.figure(figsize=(9.8,5), dpi=100)\n",
    "    \n",
    "    for i, plot_type in enumerate(['Decision Boundary']):\n",
    "        plt.subplot(1,2,i+1)\n",
    "\n",
    "        mesh_step_size = 0.5  # step size in the mesh\n",
    "        x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "        y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "        x_max = 110\n",
    "        y_max = 60\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size), np.arange(y_min, y_max, mesh_step_size))\n",
    "        if i == 0:\n",
    "            Z = fitted_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = np.sign(Z)\n",
    "        else:\n",
    "            try:\n",
    "                Z = fitted_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "            except:\n",
    "                plt.text(0.4, 0.5, 'Probabilities Unavailable', horizontalalignment='center',\n",
    "                     verticalalignment='center', transform = plt.gca().transAxes, fontsize=12)\n",
    "                plt.axis('off')\n",
    "                break\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.scatter(X[y==0,0], X[y==0,1], alpha=0.4, label='Edible', s=5)\n",
    "        plt.scatter(X[y==1,0], X[y==1,1], alpha=0.4, label='Posionous', s=5)\n",
    "        plt.imshow(Z, interpolation='nearest', cmap='RdYlBu_r', alpha=0.15, \n",
    "                   extent=(x_min, x_max, y_min, y_max), origin='lower')\n",
    "        plt.title(plot_type)\n",
    "        plt.gca().set_aspect('equal');\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.08, wspace=0.02)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_data = np.load(\"ridge_vs_svm_data_train.npy\")\n",
    "X_train = train_data[:, 1:]\n",
    "y_train = train_data[:, 0]\n",
    "\n",
    "test_data= np.load(\"ridge_vs_svm_data_train.npy\")\n",
    "X_test = test_data[:, 1:]\n",
    "y_test = test_data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the training data to get a sense of the distribution. Note the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], alpha=0.4, s=5)\n",
    "plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], alpha=0.4, s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code below to run a __linear__ svm to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = None # your trained model (as trained by scipy)\n",
    "y_pred = None # the prediction of your trained model on the testing data\n",
    "\n",
    "########## Your beautiful code starts here ##########\n",
    "# todo: Write code to train an SVM, and generate prediction y_pred.\n",
    "# Optional: Try using a linear kernel and a polynomial kernel.  How does the value of C matter? \n",
    "\n",
    "\n",
    "########## Your beautiful code ends here ##########\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print(\"Test Accuracy: {}\".format(accuracy)) \n",
    "\n",
    "plot_boundry(X_test, y_test, fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code below to run ridge regression to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = None # your trained model (as trained by scipy)\n",
    "y_pred_sign = None # the prediction of your trained model on the testing data\n",
    "\n",
    "# convert the labels from 0 and 1 to -1 and 1\n",
    "y_train_sign = np.array(y_train)\n",
    "y_test_sign = np.array(y_test)\n",
    "y_train_sign[y_train_sign == 0] = -1\n",
    "y_test_sign[y_test_sign == 0] = -1\n",
    "\n",
    "# for the regularization parameter lambda, you can try something around 0.1 :)\n",
    "# Optional: try choosing different parameters\n",
    "llambda = 0.1\n",
    "\n",
    "########## Your beautiful code starts here ##########\n",
    "# todo: train a fitted_model and run prediction to generate y_pred_sign\n",
    "\n",
    "########## Your beautiful code ends here ##########\n",
    "\n",
    "accuracy = accuracy_score(y_pred_sign, y_test_sign)\n",
    "print(\"Test Accuracy: {}\".format(accuracy))\n",
    "\n",
    "plot_boundry(X_test, y_test, fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Do We See SVM Outperforming Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we saw that SVM outperforms ridge regression because SVM is more robust to outliers. The data was actually synthetically generated from two Gaussians --- but remember the two outliers? Can you see how they are impacting the classifer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Data was produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Try changing the positions of the outliers to see how they impact the performanace\n",
    "\n",
    "n = 100\n",
    "cov = np.eye(2) * 20\n",
    "\n",
    "pos = np.hstack([\n",
    "    np.ones(n).reshape([-1, 1]),\n",
    "    np.random.multivariate_normal([5, 5], cov, size=n),\n",
    "])\n",
    "neg = np.hstack([\n",
    "    np.zeros(n).reshape([-1, 1]),\n",
    "    np.random.multivariate_normal([-5, -5], cov, size=n),\n",
    "])\n",
    "\n",
    "syn = np.vstack([pos, neg])\n",
    "\n",
    "outliers = np.array([\n",
    "    [0, 80, 50,],\n",
    "    [0, 100, 50,],\n",
    "])\n",
    "syn = np.vstack([pos, neg, outliers])\n",
    "np.random.shuffle(syn)\n",
    "np.save(\"ridge_vs_svm_data_train.npy\", syn)\n",
    "\n",
    "\n",
    "pos_test = np.hstack([\n",
    "    np.ones(n).reshape([-1, 1]),\n",
    "    np.random.multivariate_normal([5, 5], cov, size=n),\n",
    "])\n",
    "neg_test = np.hstack([\n",
    "    np.zeros(n).reshape([-1, 1]),\n",
    "    np.random.multivariate_normal([-5, -5], cov, size=n),\n",
    "])\n",
    "\n",
    "syn = np.vstack([pos_test, neg_test])\n",
    "\n",
    "np.random.shuffle(syn)\n",
    "np.save(\"ridge_vs_svm_data_test.npy\", syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spring 2019: Mong H. Ng, Prof. Ranade <br>\n",
    "Plotting function from https://github.com/devssh/svm/blob/master/SVM%20Python/Classifier%20Visualization.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
