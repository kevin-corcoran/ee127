{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Interpreting the data matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(a) Suppose we want to compute a vector that contains the mean value for     each feature. What is the length of the vector containing mean value of the     features?  Which of the following python commands will give us the mean valu    e of the features:**  \n",
    "* feature\\_means = numpy.mean(X, axis = 0)  \n",
    "* feature\\_means = numpy.mean(X, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer:** \n",
    "$\n",
    "\\boxed{\\text{feature_means = numpy.mean(X, axis = 0)}} \\\\\n",
    "$   \n",
    "And returns a $\\boxed{1 \\times m}$ array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(b) Suppose we want to compute the standard deviation of each feature. What is the dimension of the vector containing standard deviation of the features? Which of the following python commands will give us the standard deviation of the features:**  \n",
    "* feature\\_stddevs = numpy.std(X, axis = 0)  \n",
    "* feature\\_stddevs = numpy.std(X, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer:** \n",
    "$\n",
    "\\boxed{\\text{feature_stddevs = numpy.std(X, axis = 0)}} \\\\\n",
    "$   \n",
    "and returns a $\\boxed{1 \\times m}$ array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(c) Suppose we want every feature *centered*, i.e. the feature is zero mean. How would you achieve this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer:**  \n",
    "Subtract the mean of each feature from each feature.  \n",
    "\n",
    "**proof:**  \n",
    "If the mean of feature $j$ is $\\overline x_j$.  \n",
    "Then the new mean $\\overline x_j'$ is:  \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\overline x_j' & =  \\frac{\\sum_i^n \\left( X_{ij} - \\overline x_j \\right) }{  n}  && \\text{summing over the $j^{th}$ column (feature)} \\\\\n",
    "    & = \\overline x_j - n \\frac{\\overline x_j} {n} \\\\\n",
    "    & = 0\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(d) Suppose we want every feature *standardized* (i.e the feature is zero mean and has unit variance.) How would you achieve this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer:**  \n",
    "Divide each feature by the standard deviation of that feature.\n",
    "\n",
    "**proof:**  \n",
    "Using the newly centered features (zero mean)\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma ^2 & =  \\frac{ \\sum_k^n \\left( X_{kj} - \\overline x_j ' \\right) }{n} \\\\\n",
    "    & = \\frac{\\sum_k^n X_{kj}}{n} \\\\\n",
    "(\\sigma^{2})' & = \\frac{\\sum_k^n \\frac{X_{kj}}{\\sigma^2}}{n} && \\text{The new variance} \\\\\n",
    "    & = \\frac{1}{\\sigma^2} \\frac{\\sum_k^n X_{kj}}{n} \\\\\n",
    "    & = \\frac{1}{\\sigma^2} \\sigma^2 \\\\\n",
    "    & = 1\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(e) Another metric of interest is the covariance matrix, which tells us h    ow different features are related to each other. What is the size of the cov    ariance matrix?:**\n",
    "* $n \\times n$\n",
    "* $m \\times m.$  \n",
    "\n",
    "*Hint:* Is the number of features $m$ or $n$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer:**  \n",
    "$\\boxed{m \\times m}$ Since covariance measures the variance *between* features, and there are $m$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(f) For rest of the problem assume that the data matrix is centred so eve    ry feature is zero mean. Let $C$ denote the covariance matrix. Show that $C$     can be represented in the following ways:**  \n",
    "\\begin{align*}\n",
    "    C &= \\frac{X^T X}{n} \\\\\n",
    "    C &= \\frac{1}{n}\\sum\\limits_{i=1}^n \\vec{x}_i \\vec{x}_i^T.\n",
    "\\end{align*}\n",
    "#### Recall that $\\vec{x}_i^T$ is the $i$th row of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the definition of the covariance matrix the second identity directly follows from a centered data matrix**\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "& \\text{Define } C \\doteq \\frac{1}{n}\\sum_{i=1}^n (x_i - \\overline x_i) (x_i - \\overline x_i)^T \\\\\n",
    "& \\text{Then if data matrix $X$ is zero mean (centered)} \\\\\n",
    "& \\text{Then} \\\\\n",
    "& C = \\boxed{\\frac{1}{n}\\sum_{i=1}^n x_i  x_i^T}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\\newcommand{\\vertbar}{\\rule[-1ex]{0.5pt}{2.5ex}}$\n",
    "$\\newcommand{\\horzbar}{\\rule[.5ex]{2.5ex}{0.5pt}}$  \n",
    "$\n",
    "\\begin{align*}\n",
    "& \\text{ Then if we let } X = \\left( \\begin{array}{ccc} \\horzbar & x_1^T & \\horzbar \\\\ \n",
    "                              \\horzbar & 0 & \\horzbar  \\\\\n",
    "                               & \\vdots & \\\\\n",
    "                               \\horzbar & 0 & \\horzbar \n",
    "                                                       \\end{array}  \\right)\n",
    "+ \\left( \\begin{array}{ccc} \\horzbar & 0 & \\horzbar \\\\ \n",
    "                              \\horzbar & x_2^T & \\horzbar  \\\\\n",
    "                               & \\vdots & \\\\\n",
    "                               \\horzbar & 0 & \\horzbar \n",
    "                                                       \\end{array}  \\right)\n",
    "+ \\dots\n",
    "+ \\left( \\begin{array}{ccc} \\horzbar & 0 & \\horzbar \\\\ \n",
    "                              \\horzbar & 0 & \\horzbar  \\\\\n",
    "                               & \\vdots & \\\\\n",
    "                               \\horzbar & x_n^T & \\horzbar \n",
    "                                                       \\end{array}  \\right) \\\\\n",
    "& \\text{Then its easy to see how} \\\\\n",
    "& \\boxed{ \\frac{X^TX}{n} = \\frac{1}{n}\\sum_{i=1}^n x_i  x_i^T}\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(g) Let the $(i,j)$ entry of $C$ ($c_{ij}$) denote the covariance between     feature $i$ and feature $j$. Then which of the following is true?**\n",
    "* #### $c_{ij} = \\frac{1}{m}(\\vec{x}^i)^\\top \\vec{x}^j$\n",
    "* #### $c_{ij} = \\frac{1}{n}(\\vec{x}^i)^\\top \\vec{x}^j$.\n",
    "#### Recall that $\\vec{x}^i$ is the $i$th column of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\frac{X^TX}{n} = \\frac{1}{n} \\begin{pmatrix}\n",
    "                    \\horzbar & (x^{1})^T & \\horzbar \\\\\n",
    "                             &  \\vdots   &          \\\\\n",
    "                    \\horzbar & (x^{m})^T & \\horzbar \\\\\n",
    "                \\end{pmatrix}\n",
    "                \\begin{pmatrix}\n",
    "                    \\vertbar &           & \\vertbar \\\\\n",
    "                       x^1   &  \\vdots   &   x^m    \\\\\n",
    "                    \\vertbar &           & \\vertbar \\\\\n",
    "                \\end{pmatrix}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "Then its easy to see that \n",
    "#### $c_{ij} = \\boxed{ \\frac{1}{n}(\\vec{x}^i)^\\top \\vec{x}^j }$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(h) Recall that our data points are the rows of $X$ and these lie in a $m    $-dimensional space. Suppose we are interested in taking the projection of t    he points onto a one-dimensional subspace  in $\\mathbb{R}^m$ spanned by the     unit vector $u$.  Sometimes this is referred to informally  as ``Projecting     points along direction $u$''. Then which of the following is true:**  \n",
    "* $\\vec{u} \\in \\mathbb{R}^m$\n",
    "* $\\vec{u} \\in \\mathbb{R}^n$\n",
    "#### *Hint: Think about how many points we have and what dimension a single     point lies in.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer:**  \n",
    "$\\vec{u} \\in \\mathbb{R}^m$  \n",
    "Since we are looking for a unit vector in $\\mathbb{R}^m$ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(i) Note there are three different interpretations of the term ``projecti    on'' and these are used interchangeably with abuse of notation which can mak    e it confusing at times.\\\\ Consider vectors $\\vec{a}$ and $\\vec{b}$ in $\\mathbb{R}^n    $. Let $\\vec{b}$ be unit norm (i.e $\\vec{b}^T \\vec{b} = 1$). Then we have:**  \n",
    "* The $\\textbf{vector projection}$ of $\\vec{a}$ on $\\vec{b}$ is given     by $(\\vec{a}^T \\vec{b})\\vec{b}$. Note that is a vector in $\\R n$.\n",
    "* The $\\textbf{scalar projection}$ of $\\vec{a}$ on $\\vec{b}$ is given     $\\vec{a}^T \\vec{b}$. This is a scalar but can take both positive and nega    tive values.\n",
    "* The $\\textbf{projection length}$ of $\\vec{a}$ on $\\vec{b}$ is given     by $|\\vec{a}^T \\vec{b}|$, and is the absolute value of the scalar projec    tion.  \n",
    "\n",
    "#### Recall that our data points are the rows of $X$. Suppose we want to obtain a     column vector, $\\vec{z} \\in \\mathbb{R}^n$ containing scalar projections of points a    long the direction given by the unit vector $\\vec{u}$. Show that this is giv    en by,  \n",
    "\n",
    "\\begin{equation*}\n",
    "    z = X\\vec{u}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**begin question**  \n",
    "By definition of the scalar projection:  \n",
    "projection $x_i \\in \\mathbb{R}^m$ on u is given by  \n",
    "$x_i^Tu$ which is the $i^{th}$ column of $z$ in  \n",
    "\n",
    "$z = Xu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(j) Suppose we treat $\\vec{z} = (z_1, z_2, \\dots. z_n)$ as samples of a     random variable $Z \\in \\mathbb{R}$ corresponding to the scalar projection along di    rection $\\vec{u}$.  We are interested in calculating empirical variance of t    he scalar projections. Show that this can be calculated as**  \n",
    "\\begin{equation*}\n",
    "    \\sigma^2_z = \\frac{1}{n} \\vec{u}^T X^T X \\vec{u} = \\vec{u}^T C     \\vec{u}.\n",
    "\\end{equation*}\n",
    "#### *Hint: The empirical variance is given by, $\\sigma^2_z = \\frac{1}{n}\\sum\\limits_{i=1}^n(z_i - \\mu_z)^2$, where  $\\mu_z = \\frac{1}{n}\\sum \\limits_{i=    1}^n z_i$, is the empirical mean. Recall that $X$ is assumed to be centered.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin**\n",
    "\n",
    "First note that   \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\mu_z & = \\sum\\limits_{x=1}^n z_i \\\\\n",
    "    & = \\sum\\limits_{x=1}^n x_i^T u \\\\\n",
    "    & = \\left( \\sum\\limits_{x=1}^n x_i^T \\right) u \\\\\n",
    "    & = 0 && \\text{Since features are zero mean}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "So  \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma^2_z & = \\frac{1}{n}\\sum\\limits_{i=1}^n z_i^2 \\\\\n",
    "    & = \\frac{1}{n} \\sum\\limits_{i=1}^n (x_i^T u)^2 \\\\\n",
    "    & = \\frac{1}{n} \\sum\\limits_{i=1}^n u^Tx_i(x_i^T u) \\\\\n",
    "    & = \\frac{1}{n} u^T X^T X u && \\text{by previous part} \\\\\n",
    "    & = u^T C u\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Computation and Geometric Interpretation of Singular Value Decomposition (SVD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Consider the $2 \\times 2$ matrix\n",
    "##### $A = \\frac{1}{\\sqrt{10}} \\left( \\begin{array}{c} 2 \\\\ 1 \\end{array}\\right)\n",
    "\\left( \\begin{array}{cc} 1 & -1 \\end{array}\\right)\n",
    "+ \\frac{2}{\\sqrt{10}}\n",
    "\\left( \\begin{array}{c} -1 \\\\ 2 \\end{array}\\right)\n",
    "\\left( \\begin{array}{cc} 1 & 1 \\end{array}\\right).\n",
    "$  \n",
    "\n",
    "#### **(a) What is an SVD of $A$? Express it as $A = U S V^T$, with $S$ the di    agonal matrix of singular values ordered in decreasing fashion. Make sure to     check all the properties required for $U,S,V$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since A is already written as a combination of a sum of diads $A = \\sum a_i p_i q_i^T$, and noting $p_i$'s and $q_i$'s are already orthogonal,  \n",
    "  we simple need to normalize these vectors.  \n",
    "  \n",
    "$\n",
    "\\text{We want to find this form:}\n",
    "\\begin{align}\n",
    "A & = \\sum_{i=1}^{2} \\sigma_i u_i v_i^T \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "A & = \\sqrt{5}\\sqrt{2}\\frac{1}{\\sqrt{10}} \\left( \\begin{array}{c} \\frac{2}{\\sqrt{5}} \\\\ \\frac{1}{\\sqrt{5}} \\end{array}\\right)\n",
    "\\left( \\begin{array}{cc} \\frac{1}{\\sqrt{2}} & \\frac{-1}{\\sqrt{2}} \\end{array}\\right)\n",
    "+ \\sqrt{5}\\sqrt{2}\\frac{2}{\\sqrt{10}}\n",
    "\\left( \\begin{array}{c} \\frac{-1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{array}\\right)\n",
    "\\left( \\begin{array}{cc} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{array}\\right) \\\\\n",
    "     & = \\left( \\begin{array}{c} \\frac{2}{\\sqrt{5}} \\\\ \\frac{1}{\\sqrt{5}} \\end{array}\\right)\n",
    "\\left( \\begin{array}{cc} \\frac{1}{\\sqrt{2}} & \\frac{-1}{\\sqrt{2}} \\end{array}\\right)\n",
    "+ 2\n",
    "\\left( \\begin{array}{c} \\frac{-1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{array}\\right)\n",
    "\\left( \\begin{array}{cc} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{array}\\right) \\\\ \\\\\n",
    "\\therefore A & = \\boxed{ \\begin{pmatrix}   \\frac{-1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}}  \\\\ \\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{5}}  \\end{pmatrix}\n",
    "    \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}\n",
    "    \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{-1}{\\sqrt{2}}\\end{pmatrix} } & \\text{rearranging}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(b) Find the semi-axis lengths and principal axes of the ellipsoid**  \n",
    "\n",
    "##### ${\\cal E}(A) = \\left\\{ Ax ~: x \\in \\mathbb{R}^{2} , \\;\\; \\|x\\|_2 \\le 1 \\right\\}.$  \n",
    "\n",
    "##### *Hint:* Use the SVD of $A$ to show that every element of ${\\cal E}(A)$ i    s of the form $y = U\\bar{y}$ for some element $\\bar{y}$ in ${\\cal E}(S)$. Th    at is, ${\\cal E}(A) = \\{ U\\bar{y} \\::\\: \\bar{y} \\in {\\cal E}(S) \\}$.  (In ot    her words the matrix $U$ maps ${\\cal E}(S)$ into the set ${\\cal E}(A)$.) The    n analyze the geometry of the simpler set ${\\cal E}(S)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "Ax &= y \\\\\n",
    "U\\Sigma V^T x & = y && \\text{for $\\|x\\|_2 \\leq 1$} \\\\\n",
    "U\\Sigma y' & = y && \\text{letting $y' = V^T x$} \\\\\n",
    "\\end{align*}\n",
    "$  \n",
    "noting that $V^T$ is a norm preserving isometry\n",
    "and since the $V^T$ spans $\\mathbb{R}^2$ we can pick a basis for $Range(V^T)$ as $\\{ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\n",
    "\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}  \\}$  \n",
    "then seeing that this basis gets scaled as $\\{ \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix},\n",
    "\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}  \\}$ by $\\Sigma$  \n",
    "Finally, this set gets rotated in the direction of $\\begin{pmatrix} \\frac{-1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{pmatrix}$ and\n",
    "$\\begin{pmatrix} \\frac{2}{\\sqrt{5}} \\\\ \\frac{1}{\\sqrt{5}} \\end{pmatrix}$, respectively.\n",
    "\n",
    "So the semi-axis length is the length of $1 \\times \\begin{pmatrix} \\frac{2}{\\sqrt{5}} \\\\ \\frac{1}{\\sqrt{5}} \\end{pmatrix}$ which equals 1  \n",
    "and the principal axis are the span of $\\begin{pmatrix} \\frac{-1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{pmatrix}$ with length equal to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(c) Same question when we append a column after the last column of $A$, that is,     $A$ is replaced with $\\tilde{A} = [A,0] \\in \\mathbb{R}^{2 \\times 3}$. I    nterpret geometrically your result.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the same. Since the column rank of $A$ does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(d) Same question when we append a row after the last row of $A$, that is,     $A$ is replaced with $\\tilde{A} = [A^T,0]^T \\in \\mathbb{R}^{3 \\times 2}$. I    nterpret geometrically your result.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the same. Since the column rank of $A$ does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. PCA and low-rank compression**\n",
    "**($P_1$) Finding a line going through the origin that maximizes the variance of the points projected\n",
    "on the line.**  \n",
    "**($P_2$) Finding a line going through the origin that minimizes the sum of squares of the distances\n",
    "from the points to their projections;**  \n",
    "**($P_3$) Finding a rank-one approximation to the data matrix.**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(a) Consider the problem of projecting a point $\\vec{x}$ on a line ${\\cal L} = \\{\\vec{x}_0 + v \\vec{u} \\::\\: v \\in \\mathbb{R}\\}$, with $\\vec{x}_0 \\in \\mathbb{R}^{m}$,  $\\vec{u}^T\\vec{u}=1$, given.**\n",
    " \n",
    "#### Show that the projected point $\\vec{z}$ is given by  \n",
    "#### $$\\vec{z} = \\vec{x}_0+v^* \\vec{u},$$ \n",
    "#### where we define  \n",
    "#### $$v^* = (\\vec{x}-\\vec{x}_0)^\\top\\vec{u},$$ \n",
    "#### and that the minimal squared distance $\\|\\vec{z}-\\vec{x}\\|_2^2$ is equal to $\\|\\vec{x}-\\vec{x}_0\\|_2^2 - ((x-x_0)^Tu)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll draw a picture  \n",
    "* Showing that $z = x_0 + ((x-x_0)^Tu)u$  \n",
    "\n",
    "\n",
    "<img src=\"offcenterproj-1.png\" width = \"500\" align = \"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the other part:\n",
    "$\n",
    "\\begin{align*}\n",
    "\\|z-x\\|^2 & = \\langle z - x , z - x \\rangle \\\\\n",
    "    & = \\langle x_0-x + (x^Tu)u - (x_0^Tu)u, x_0-x + (x^Tu)u - (x_0^Tu)u \\rangle && \\text{substituting $z = x_0 + ((x-x_0)^Tu)u$} \\\\\n",
    "    & = \\|x-x_0\\|^2 + \\langle (x^Tu)u - (x_0^Tu)u, (x^Tu)u - (x_0^Tu)u \\rangle \\\\\n",
    "    & = \\|x-x_0\\|^2 + ((x^Tu)u^T - (x_0^Tu)u^T)((x^Tu)u - (x_0^Tu)u) \\\\\n",
    "    & = \\|x-x_0\\|^2 + (x^Tu)^2 - 2(x^Tu)(x_0^Tu) + (x_0^Tu)^2 \\\\\n",
    "    & = \\|x-x_0\\|^2 + (x^Tu - x_0^Tu)^2\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(b) Show that problems *$P_1$, $P_2$* are equivalent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the line goes through the origin then projecting $x$ onto it yields $z = (x^Tu)u$\n",
    "and $\\|z-x\\|^2 = \\|x\\|^2 - (x^Tu)^2$\n",
    "\n",
    "So if we assume the distance is minimized then $\\|z-x\\|^2 = \\|x\\|^2 - (x^Tu)^2 = 0$  \n",
    "then $\\|x\\| = x^Tu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. PCA and face analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(a) Show that the direction of maximal variation is the direction of the eigenvector corresponding to the largest eigenvalue of the matrix $\\tilde{X}^{\\top}\\tilde{X}$**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that the covariance matrix can be written as $\\frac{1}{n}X^TX$\n",
    "\n",
    "and since the covariance matrix is symmetric, it can be orthogonally diagonalized.\n",
    "\n",
    "and since we want to maximize the mean squared variation along some direction $z$\n",
    "\n",
    "if we let $z$ be the eigenvector with largest eigenvalue (and ignoring the $\\frac{1}{n}$ term)\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "z^TX^T Xz & = \\lambda_{largest}z^Tz \\\\\n",
    "    & = \\lambda_{largest}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(b) How is this related to the singular value decomposition of XÌƒ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X = U\\Sigma V^T$  \n",
    "Then $X^T = V\\Sigma U^T$  \n",
    "And $X^T X = V\\Sigma ^2V^T$ which is (almost) exactly the eigendecomposition of the covariance matrix. (need a $\\frac{1}{n}$)\n",
    "So the singular values squared is the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly the same as **(a)**..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(d)-(f)**  \n",
    "$\\dots$ should be apparent by screenshots below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(g)**  \n",
    "<img src=\"one.png\" width = \"500\" align = \"left\">  \n",
    "<img src=\"other.png\" width = \"500\" align = \"right\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Eigenvectors of a symmetric matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\vec{p},\\vec{q} \\in \\mathbb{R}^{n}$ be two linearly independent vectors, with     unit norm ($\\|\\vec{p}\\|_2 = \\|\\vec{q}\\|_2 = 1$). Define the symmetric matri    x $A\\doteq \\vec{p}\\vec{q}^T+\\vec{q}\\vec{p}^T$. In your derivations, it     may be useful to use the notation $c \\doteq  \\vec{p}^T\\vec{q}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(a) Show that $\\vec{p}+\\vec{q}$ and $\\vec{p}-\\vec{q}$ are eigenvectors of     $A$, and determine the corresponding eigenvalues.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "A (p + q) & = pq^Tp + pq^Tq + qp^Tp + qp^Tq \\\\\n",
    " & = p(q^Tp) + p(q^Tq) + q(p^Tp) + q(p^Tq) & \\text{$p^Tq$, $q^Tq$, etc are scalars}\\\\\n",
    " & = (q^Tp + q^Tq)p +(p^Tp + p^Tq)q \\\\\n",
    " & = (q^Tp + 1)(p + q) & \\text{$p$ and $q$ have unit norm and $q^Tp = p^Tq$}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\\therefore p+q$ is an eigenvector with eigenvalue $q^Tp + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "A (p - q) & = pq^Tp - pq^Tq + qp^Tp - qp^Tq \\\\\n",
    " & = p(q^Tp) - p(q^Tq) + q(p^Tp) - q(p^Tq) & \\text{$p^Tq$, $q^Tq$, etc are scalars}\\\\\n",
    " & = (q^Tp - q^Tq)p -(-p^Tp + p^Tq)q \\\\\n",
    " & = (q^Tp - 1)(p - q) & \\text{$p$ and $q$ have unit norm and $q^Tp = p^Tq$}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$\\therefore p-q$ is an eigenvector with eigenvalue $q^Tp - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(b) Determine the nullspace and rank of A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nullspace of $A$ is the set of vectors $x$ such that $Ax = 0$  \n",
    "\n",
    "So if $Ax = (pq^T + qp^T)x = 0$ then $x$ is orthogonal to both $p$ and $q$ (else $p$ and $q$ are not linearly independent)\n",
    "\n",
    "and there are $n-2$ such possible vectors. Therefore the rank of $A$ is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(c) Find an eigenvalue decomposition of A, in terms of p, q. Hint: use the previous two parts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "A = \\begin{pmatrix} p + q & p - q \\end{pmatrix} \n",
    "    \\begin{pmatrix} q^Tp + 1 & 0 \\\\ 0 & q^Tp -1 \\end{pmatrix}\n",
    "    \\begin{pmatrix} p^T + q^T \\\\ p^T - q^T \\end{pmatrix}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(d) What is the answer to the previous part if p, q are not normalized? Write A as a function p, q and their norms and the new eigenvalues as a function of p, q and their norms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p' = \\frac{p}{\\|p\\|}$ and $q' = \\frac{q}{\\|q\\|}$  \n",
    "So $A' = p'q'^T + q'p'^T$  \n",
    "Then $A'$ has eigenvectors $p'+q'$ and $p' - q'$ with eigenvalues $q'^Tp' + 1$ and $q'^Tp' - 1$ respectively  \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "A' & = \\frac{1}{\\|p\\| \\|q\\|} \\left( pq^T + qp^T \\right) \\\\\n",
    "A & = \\|p\\| \\|q\\| A' \\\\\n",
    "A(p' + q') & = \\|p\\| \\|q\\| (q'^Tp' + 1) (p' + q') \\\\\n",
    "A \\left( \\frac{p}{\\|p\\|} + \\frac{q}{\\|q\\|} \\right)\n",
    "    & = (q^Tp + \\|p\\| \\|q\\|) \\left( \\frac{p}{\\|p\\|} + \\frac{q}{\\|q\\|} \\right) \\\\\n",
    "    \\dots & \\text{similarly for $p'-q'$}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "So $A$ has eigenvectors $\\frac{p}{\\|p\\|} + \\frac{q}{\\|q\\|}$ and $\\frac{p}{\\|p\\|} - \\frac{q}{\\|q\\|}$ with eigenvalues $q^Tp + \\|p\\| \\|q\\|$ and $q^Tp - \\|p\\| \\|q\\|$ respectively\n",
    "\n",
    "And $\\therefore$ the eigendecomposition of A is:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "A = \\begin{pmatrix} \\frac{p}{\\|p\\|} + \\frac{q}{\\|q\\|} & \\frac{p}{\\|p\\|} - \\frac{q}{\\|q\\|} \\end{pmatrix} \n",
    "    \\begin{pmatrix} q^Tp + \\|p\\| \\|q\\| & 0 \\\\ 0 & q^Tp - \\|p\\| \\|q\\| \\end{pmatrix}\n",
    "    \\begin{pmatrix} \\frac{p^T}{\\|p\\|} + \\frac{q^T}{\\|q\\|} \\\\ \\frac{p^T}{\\|p\\|} - \\frac{q^T}{\\|q\\|} \\end{pmatrix}\n",
    "\\end{align*}\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
